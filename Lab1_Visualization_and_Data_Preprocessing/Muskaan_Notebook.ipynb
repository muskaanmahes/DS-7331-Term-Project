{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907a03e2-70d2-409e-8cc5-55c833be8090",
   "metadata": {},
   "source": [
    "# Lab One: Visualization and Data Preprocessing\n",
    "# by Nino Castellano, Aayush Dalal, Chloe Prowse, Muskaan Mahes\n",
    "\n",
    "# 1. Business Understanding\n",
    "# The Student Placement Dataset from Kaggle was collected and analyzed to determine whether students’ results were sufficient to obtain a job offer. The dataset contained over 50,000 records consisting of academic, technical, and soft-skill attributes that can influence the outcome of being placed or not. Therefore, the primary purpose of the dataset is to help students and educational institutions understand which factors are crucial for achieving a successful placement outcome. Using placement results is important, as they can provide real-world examples that can serve as a template for assessing how well a student is prepared for the job market.\n",
    "\n",
    "# The main objective of this analysis is to explore the relationships between students’ key features, and their placement outcome to identify important insights. Therefore, the target variable from the training set, placement status, was used to measure the effectiveness of the student outcome.\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "#Importing Train Dataset for Intial EDA\n",
    "df = pd.read_csv(\"/Users/muskaanmahes/Downloads/placement_train.csv\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbc759-d73b-481d-945c-0585d196ad22",
   "metadata": {},
   "source": [
    "# 2. Data Understanding\n",
    "\n",
    "# This dataset contains 14 attributes that describe a student profile. The variable Student_ID is a unique identifier, while Age is a ratio-scale numeric value. The categorical features include Gender, Degree, Branch, and Placement_Status, which are all nominal variables. Additionally, there are academic variables which are all on a ratio-scale: Internships, Projects, Certifications, Backlogs, and CGPA, which is on an interval scale. Skill-related attributes such as Coding_Skills, Communication_Skills, Aptitude_Test_Score, and Soft_Skills_Rating are either on the ordinal or interval scale, ranging from 1 to 10. Therefore, the dataset has numerical, categorical, and ratio features that are crucial for analyzing a student's performance. \n",
    "\n",
    "#type of data each attribute\n",
    "df.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861cf72c-c282-4902-b3ed-5d0822558ebb",
   "metadata": {},
   "source": [
    "# After analyzing the dataset, no missing or duplicated data was identified. All 15 columns were checked for missing values, though it returned an empty DataFrame, hence indicating that there are no incomplete values. Additionally, the dataset contained zero duplicate rows, demonstrating that each student entry is unique. Therefore, the overall data is clean and well-defined. \n",
    "\n",
    "#missing values\n",
    "#to check for any nulls\n",
    "rows_with_nulls = data[data.isnull().any(axis=1)]\n",
    "print(\"Rows with null values: \")\n",
    "print(rows_with_nulls)\n",
    "\n",
    "#duplicate data\n",
    "number_duplicates = data.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {number_duplicates}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7370e-8537-4120-9590-91d562601ec5",
   "metadata": {},
   "source": [
    "# However, after calculating the interquartile range (IQR) for each numeric variable, several columns were flagged to have potential outliers. Specifically, there were 153 outliers in CGPA, 1652 in Internships, 1130 in Projects, and 4354 in Soft_Skills_Tating. Additionally, these outliers are not data entry errors as they were found through the IQR method. For example, having a high CGPA, such as 9.80, can indicate a strong academic performance, and having multiple internships or projects is realistic for motivated students. Therefore, the outliers can skew the metrics; we should not remove them. Instead, we could extend the bounds to reduce extreme values. \n",
    "\n",
    "#are there outliers\n",
    "numeric_column = data.select_dtypes(include = ['float64', 'int64']).columns\n",
    "\n",
    "for col in numeric_column:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outlier = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "\n",
    "    print(f\"\\nOutliers in '{col}': \")\n",
    "    print(outlier[[col]])\n",
    "    print(f\"Total outliers in '{col}': {outlier.shape[0]}\")\n",
    "\n",
    "\n",
    "\n",
    "# The violin and strip plots depict the distribution and their respective outliers within the four numerical features. In the CGPA plot, even though high values like 9.8 are flagged as outliers, they represent high academic performances. The Intership plot ranges from 0 to 3, with value of 3 considered to be outliers, though they can still reflect real-world scenarios. In the Projects plot, the data is mainly concentrated between ranges 2 and 5, while ranges 1 and 6 are flagged as outliers, which may depict lower or higher engagment. Lastly, the Soft_Skills_Ratings ranged from 1 to 10, and have outliers on both ends. Therefore, these outliers are statistically significant and may not be erroneous. \n",
    "\n",
    "#visualizing outliers\n",
    "#columns that had outliers\n",
    "column_outlier = ['CGPA', 'Internships', 'Projects', 'Soft_Skills_Rating']\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "#violin and strip plots\n",
    "for i, col in enumerate(column_outlier, 1):\n",
    "    plt.subplot(2,2,i)\n",
    "    sns.violinplot(x=data[col], inner=None, color='gray')\n",
    "    sns.stripplot(x=data[col], color = 'red', size=2, jitter=True)\n",
    "    plt.title(f'Outlier Visualization of {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f40ae1-9f84-462c-b120-08e9ffa93278",
   "metadata": {},
   "source": [
    "\n",
    "#Dimension Reduction: LDA + Logistic Regression\n",
    "#defining the features and target variable\n",
    "X = df[['CGPA', 'Internships', 'Projects', 'Soft_Skills_Rating']]\n",
    "y = df['Placement_Status'].astype('category').cat.codes  # Converts to 0 (Not Placed), 1 (Placed)\n",
    "\n",
    "# Train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "#Standardize features\n",
    "scale = StandardScaler()\n",
    "x_train_std = scale.fit_transform(x_train)\n",
    "x_test_std = scale.transform(x_test)\n",
    "\n",
    "#applying LDA and using 1 component because of the outcome\n",
    "lda = LDA(n_components=1)\n",
    "x_train_lda = lda.fit_transform(x_train_std, y_train)\n",
    "x_test_lda = lda.transform(x_test_std)\n",
    "\n",
    "#training with logistic model\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_lda, y_train)\n",
    "\n",
    "# To implement dimension reduction, a linear discriminant analysis (LDA) was applied to reduce the feature space to a single linear discriminant (LD1), with the target variable as Placement_Status. LDA is a supervised technique that not only reduces dimensionality but also separates the two outcomes classes. After this tra, a logistic regression classifier was trained on the data. This resulted in a plot that shows the linear boundary and the separate discriminative features. The logistic regression method was chosen due to its simplicity and effectiveness in binary classification. \n",
    "\n",
    "#plot\n",
    "plot_decision_regions(x_train_lda, y_train.to_numpy().astype(np.int_), clf=model)\n",
    "plt.xlabel(\"LD1\")\n",
    "plt.ylabel(\"Decision Boundary\")\n",
    "plt.title(\"LDA + Logistic Regression: Decision Regions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44128444-861a-44b5-ba73-fda09895448b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
